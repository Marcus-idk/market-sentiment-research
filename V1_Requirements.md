# Trading Bot Development Plan (Concise)

## Project Goal
Automated trading bot that uses LLMs for fundamental analysis. Polls data every 5 minutes, flags urgent events, and issues HOLD/SELL recommendations via scheduled LLM analysis.

## Core Strategy
- Target: Retail traders (not HFT)
- Edge: LLM scans hundreds of sources 24/7
- Scope: Monitor existing positions only (no new discovery)

## Market & Tech Specs
- Market: US equities (NYSE/NASDAQ); US-listed stocks only
- Sessions (ET): Pre 4:00â€“9:30, Regular 9:30â€“16:00, Post 16:00â€“20:00
- Timezone: Store UTC (ISO Z); convert to ET for logic; session enum {REG, PRE, POST}
- Data Sources: Finnhub, Polygon.io, SEC EDGAR, RSS, Reddit

---

## v0.1 â€” LLM Foundation âœ…
- LLM provider module (abstract base + clean provider pattern)
- Providers: GPT-5 (final decisions), Gemini 2.5 Flash (specialists)
- Async implementation + SHA-256 validation tests
- Status: Production-ready

---

## v0.2 â€” Core Infrastructure âœ…
- Data models: NewsItem, PriceData, AnalysisResult, Holdings (strict validation)
- Storage: SQLite schema (4 tables, WAL, constraints) + CRUD, URL normalization, DB-level dedup (INSERT OR IGNORE, natural PKs)
- Interfaces: Typed DataSource base classes (DataSource, NewsDataSource, PriceDataSource)
- Enums: Session, Stance, AnalysisType; Decimal precision for finance
- Tests: Model validation; CRUD + type conversions; direct SQL constraint checks
- Extras: Holdings break-even; AnalysisResult JSON validation; expert DB optimizations; URL normalization
- Files (v0.2):
```
data/
â”œâ”€â”€ __init__.py          # Clean exports
â”œâ”€â”€ base.py              # Abstract DataSource + validation
â”œâ”€â”€ models.py            # Dataclasses + enums
â”œâ”€â”€ schema.sql           # SQLite schema (WAL, constraints)
â”œâ”€â”€ storage.py           # CRUD + URL normalization
â”œâ”€â”€ API_Reference.md     # Planned data source APIs
â””â”€â”€ providers/           # Placeholder for future APIs
```
- Cost: $0

---

## v0.21 â€” Single API Integration ðŸ“¡
- Goal: Add Finnhub; local polling only
- Components: Finnhub provider (news + price), basic scheduler, config (API key)
- Success: Connects, fetches, stores locally; dedup works; manual polling
- Files (adds to v0.2):
```
data/
â”œâ”€â”€ config.py            # API keys, local env
â”œâ”€â”€ scheduler.py         # Local polling
â””â”€â”€ providers/
    â”œâ”€â”€ __init__.py
    â””â”€â”€ finnhub.py       # News + Price providers
```
- Cost: $0 (Finnhub free tier)

---

## v0.22 â€” GitHub Actions Automation â˜ï¸
- Goal: Cloud execution; 5-min cron
- Components: GH Actions workflow; commit SQLite to repo; secrets for keys
- Success: Runs every 5 min; cloud fetch; DB persists; 24/7; no local runs
- Files:
```
data/ (as v0.21)
config.py (GH secrets integration)

.github/workflows/trading-bot.yml  # 5-min polling + commit DB
```
- Cost: $0 (GH free tier)

---

## v0.23 â€” Complete Basic System ðŸŽ¯
- Goal: Second source + filtering
- Components: RSS provider; keyword filtering (urgent); enhanced scheduler; UTC standardization
- Success: Finnhub + RSS; urgent detection; cross-source dedup; end-to-end ready; unblock LLM (v0.3)
- Files (adds to v0.22):
```
data/
â”œâ”€â”€ filters.py           # is_urgent(), URGENT_KEYWORDS
â”œâ”€â”€ scheduler.py         # Multi-source
â””â”€â”€ providers/rss.py     # feedparser, pub-date compare
```
- Flow:
```
Every 5 min: scheduler â†’ providers.fetch_incremental(last_seen)
â†’ convert to models â†’ dedup â†’ storage â†’ filters.is_urgent()
â†’ urgent: trigger LLM | normal: batch for 30 min

Every 30 min: LLMs process batch â†’ update analysis_results
â†’ success: delete raw | failure: keep raw for retry
```
- Provider responsibilities: API comms; convert to models; UTC timestamps; incremental fetch
- Pattern:
```python
# Dual (news + price)
class FinnhubNewsProvider(NewsDataSource):
    async def fetch_incremental(self) -> List[NewsItem]: ...
class FinnhubPriceProvider(PriceDataSource):
    async def fetch_incremental(self) -> List[PriceData]: ...

# Single-purpose (news only)
class RSSNewsProvider(NewsDataSource):
    async def fetch_incremental(self) -> List[NewsItem]: ...
class RedditSentimentProvider(NewsDataSource):
    async def fetch_incremental(self) -> List[NewsItem]: ...
```
- Benefits: Type safety; single responsibility; shared config; independent schedules
- Cost: $0 (free tiers)

---

## v0.25 â€” Complete Data Collection ðŸ“Š (Expansion MVP)
- Goal: Add remaining sources + robustness
- Sources:
  - Polygon.io (backup market data): batch queries; 5 calls/min free
  - Reddit (PRAW): retail sentiment; ~100 queries/min non-commercial
  - SEC EDGAR: filings/insider trades; 10 req/sec; REST + XML/JSON
  - Note: SEC is stocks only (no crypto)
- Enhancements: Advanced filtering engine; retries/circuit breakers; perf/health metrics; data quality (cross-source validation, freshness, anomalies)
- Success: All 5 sources working; robust recovery; monitoring; validated data; LLM-ready
- Files (v0.25):
```
data/
â”œâ”€â”€ __init__.py          # DataSource, providers, scheduler
â”œâ”€â”€ base.py              # ABC: fetch_incremental(), validate_connection()
â”œâ”€â”€ models.py            # News, Price, Sentiment, Filing
â”œâ”€â”€ config.py            # All API keys; GH secrets
â”œâ”€â”€ storage.py           # store_items(), get_items_since()
â”œâ”€â”€ deduplication.py     # is_processed(), mark_processed()
â”œâ”€â”€ filters.py           # Keyword/ML-ready rules
â”œâ”€â”€ scheduler.py         # poll_all_sources(), error handling
â”œâ”€â”€ health_monitor.py    # health + performance
â””â”€â”€ providers/
    â”œâ”€â”€ finnhub.py      # news + price
    â”œâ”€â”€ polygon.py      # news + price
    â”œâ”€â”€ rss.py          # news only
    â”œâ”€â”€ reddit.py       # sentiment as news
    â””â”€â”€ sec_edgar.py    # regulatory news

.github/workflows/trading-bot.yml
```
- Cost: Free $0; Recommended ~$50 (Finnhub paid); Premium ~$150 (Finnhub + Polygon)

---

## v0.3+ â€” Trading Intelligence Layer (Future)
- Pipeline:
```
30-min raw batches â†’ Specialist LLMs â†’ Persistent analysis â†’ Head Trader LLM â†’ HOLD/SELL
```
- Roles: News Analyst; Sentiment Analyst; SEC Filings Analyst; Head Trader (synthesizes + holdings)
- Strategy: Sort-and-rank (not numeric scoring); update per-symbol persistent analysis
- Triggers: Urgent keywords (immediate); scheduled (30 min); cleanup on success; preserve raw on failure; head trader reads persistent analysis
- Trading logic: Portfolio tracking; signal generation; HOLD/SELL engine
- Orchestration: GH Actions; 5-min polling; 30-min LLM; SQLite in-repo; fully cloud; outputs: holdings analysis + recs
- Implementation: Clean architecture; env-based config; SQLite indexes by query patterns; structured logging; integration tests

---

## v1.0 â€” Complete Trading Bot ðŸŽ¯ (Final Target)
- Feature set: LLM providers (v0.1); data sources (v0.2+); agents/orchestration/scheduling (v0.3+); production infra (this phase)
- Infra: Rate limiting; lightweight local ML filtering; data validation; circuit breakers; monitoring/health; DB optimization; redundant failover
- Metrics: Beat buy-and-hold; 99%+ collection uptime; $10â€“$50/month; recommend-only (no auto-execution)
- Stack: Python; GitHub Actions; clean architecture; async I/O

